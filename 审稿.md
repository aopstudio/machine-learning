This paper proposes a model which enhances BERT with knowledge graph and use recalling strategy to less forgetting, namely R-BERT.
The authors use 7 typical Chinese datasets to compare R-BERT with BERT, K-BERT and ZEN, and point out that R-BERT is simple but effective.
The authors also conduct multiple ablation experiments to explore the role of each part in the pre-training model architecture based on the recall knowledge graph.
The authors acknowledge that the quality of the existing knowledge graph will directly affect the effect of R-BERT.

- Strength
1. The research problem is interesting and valuable.
2. The model is simple but effective.
3. The model has better performance than BERT, K-BERT and ZEN in both of classification tasks and NER tasks.
4. The ablation experiments show the role of each part clearly.

- Weakness
1. The paper mention the quality of knowledge graph will affect the effect of the model, but doesn't show the clear data.
2. All of examples are only Chinese datasets. The authors need to try to construct a multilingual dataset, for a broader audience.
3. The ablation experiments don't show the influence on training time by removing different parts.

- Details:
1. Fig.2 should figure out that red and green is new method clearly.
2. No need to explain Transformer's algorithm.
3. Formula (7) should define that t is training step clearly.
4. Fig.6 says Timestep, but formula(7) and formula(8) says trainning step. Which one is right?
5. Table 2 should put "K-BERT" on one line(also for "R-BERT").
6. misspelling: 
“Discription” => "Description"